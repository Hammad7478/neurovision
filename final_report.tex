\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[]{acl}
\usepackage{times}
\usepackage{latexsym}



% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Group 111 Final Project:\\NeuroVision - MRI Brain Tumor Classification}


\author{Tung Ho, Hammad Rehman, Asher Haroon \\
  \texttt{\{hot34,rehmah9,harooa5\}@mcmaster.ca} }

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
% \begin{abstract}
% \end{abstract}


\section{Introduction}

Brain tumor classification from MRI scans is a critical problem in medical imaging, as radiologists must manually analyze large numbers of scans, a process that is time-consuming, subjective, and prone to variability. Deep learning models, particularly convolutional neural networks (CNNs), have demonstrated strong potential in assisting with medical image interpretation by automatically learning spatial and structural patterns that distinguish different tumor types \citep{he2016res}. In this project, \textbf{NeuroVision}, we aim to develop a reliable deep learning system capable of classifying MRI images into four clinically relevant categories: \textit{glioma}, \textit{meningioma}, \textit{pituitary tumor}, and \textit{no tumor}. Our goals include achieving high predictive performance, ensuring stable generalization, and providing interpretable model outputs suitable for medical contexts.

Previous work has shown that CNN-based architectures fine-tuned on medical datasets can achieve strong performance when combined with appropriate augmentation strategies \citep{sajjad2019brain}. Early work on brain tumor MRI classification demonstrated that deep feature extraction generally outperforms traditional handcrafted features \citep{cheng2016brain}, reinforcing the suitability of deep models for this domain. Methods such as Focal Loss \citep{lin2017focal} have been introduced to address class imbalance, a common challenge in medical datasets, while interpretability techniques such as Grad-CAM \citep{selvaraju2017gradcam} help highlight the image regions influencing model predictions, increasing trust and transparency. Although vision transformers such as ViT \citep{dosovitskiy2020vit} represent a promising direction in computer vision, they typically require very large datasets, making CNN transfer learning more appropriate for our dataset size. Lightweight architectures such as MobileNetV2 \citep{sandler2018mobilenetv2} have also been used in medical imaging as efficient baselines for comparison.

\subsection{Influence of Related Work After the Progress Report}

After reviewing additional related work following the progress report, we did not identify new research that substantially changed our intended direction. Instead, the literature we examined was consistent with the approach we had already planned---using CNN-based transfer learning, enhancing augmentation, and applying interpretability tools. The related work primarily reinforced our existing methodology rather than altering it, confirming that our planned approach aligned with established best practices in MRI classification.




\section{Dataset}

For this project, we used the \textit{Brain Tumor MRI Dataset} from Kaggle created by Masoud Nickparvar. It contains a total of 7,023 MRI images of human brains divided into four categories: \textit{glioma}, \textit{meningioma}, \textit{pituitary}, and \textit{no tumor}. Each image is a 2D MRI scan saved as a JPEG file. Although the dataset ships with a train/test folder structure, our experiments use an explicit 80/20 stratified split created in code for training and validation; the held-out Kaggle test set is not consumed by the current training scripts.

The training set has 5,712 images and the testing set has 1,311 images in total. Table~\ref{tab:dataset} shows the number of samples in each class for both splits. While the dataset is fairly balanced, the \textit{no tumor} class has slightly more examples than the others. To prevent the model from being biased toward this class, we used class weights during training.

\begin{table}[h]
\centering
\small
\begin{tabular}{lcccc}
\hline
\textbf{Split} & \textbf{Glioma} & \textbf{Meningioma} & \textbf{Pituitary} & \textbf{No Tumor} \\
\hline
Train & 1321 & 1339 & 1457 & 1595 \\
Test  & 300  & 306  & 300  & 405  \\
\hline
Total & 1621 & 1645 & 1757 & 2000 \\
\hline
\end{tabular}
\caption{Number of MRI images per class in the training and testing sets.}
\label{tab:dataset}
\end{table}

Before feeding the images into the model, we performed some preprocessing steps. All images were resized to $224\times224$ pixels and converted to RGB format so they matched the input shape required by ResNet-50. We normalized pixel values using the mean and standard deviation from ImageNet to help the model train more efficiently. To make the model more robust, we applied random horizontal flips, small rotations up to 15 degrees, and slight brightness and contrast adjustments during training. In the ablation runs we could also disable augmentation entirely via a CLI flag.

No extra labeling or manual annotation was needed since the dataset already included folder-based labels. We double-checked that the training and testing sets were kept separate to avoid data leakage. Overall, this dataset was a good fit for our project because it was large enough for training a deep model while still being manageable to preprocess and run experiments on.

\subsection{Dataset Changes Since the Progress Report}

Our dataset remained the same as in the progress report. We did not make any changes to the data source, class labels, or overall dataset structure. Since the original dataset was already clean, labeled, and appropriately sized for our task, no modifications were necessary. All updates in our final project instead focused on model improvements, enhanced augmentation strategies, and extended evaluation rather than altering the dataset itself.


\section{Features and Inputs}

Our model takes brain MRI images as input, with each image resized to $224\times224$ pixels and converted to three RGB channels. Since we use a convolutional neural network (ResNet-50), we did not manually design features. Instead, the network automatically learns hierarchical representations such as edges, textures, and structural patterns that distinguish different tumor types. This form of representation learning is well-suited to MRI analysis because tumours exhibit subtle spatial and textural variations that CNNs can capture effectively.

We did not compute handcrafted features such as intensity histograms or shape descriptors. Instead, we relied on the pre-trained convolutional filters from ResNet-50, originally trained on ImageNet, which provide strong general-purpose visual representations that can be fine-tuned for medical imaging tasks. This makes transfer learning an appropriate choice given the moderate dataset size.

To improve generalization, we applied random horizontal flips, small rotations up to 15 degrees, and slight brightness and contrast adjustments during training. These augmentations help the model learn invariance to minor image transformations and simulate natural variability across MRI machines and acquisition conditions.

\subsection{Feature Variations for Experiments}
For the final set of experiments, we added explicit ablation flags to the training code to toggle augmentation (on/off), class weighting (on/off), color mode (RGB/grayscale), and fine-tuning depth (partial vs.\ frozen backbone). We also exposed the number of epochs and target validation accuracy via CLI so we could run faster sweeps (8-epoch cap with early stopping at 85\% validation accuracy). These controls let us systematically compare how each choice affects learned features and generalization.


\section{Implementation}

For our implementation, we used a transfer learning approach with the ResNet-50 architecture from TensorFlow’s Keras library. The model was initialized with pre-trained ImageNet weights to leverage existing feature extraction capabilities and then fine-tuned on our brain MRI dataset. We replaced the original classification layer with a custom dense layer of four output neurons representing our target classes: \textit{glioma}, \textit{meningioma}, \textit{pituitary}, and \textit{no tumor}. A softmax activation function was applied at the output layer to produce class probabilities.

Before training, we froze the lower convolutional layers to retain general feature representations such as edges and textures, while allowing the upper layers to adapt to medical image-specific features. Inputs are resized to $224 \times 224$ pixels and preprocessed using ResNet-50’s ImageNet normalization. By default the training script uses a batch size of 32, an epoch cap of 25, and early stopping at 95\% validation accuracy; the ablation sweeps override these to 8 epochs with early stopping at 85\% to shorten runs. The data is split in-code using an 80–20 stratified train/validation split; the Kaggle test set is not consumed by the current pipeline.

Our baseline model was a simple logistic regression classifier trained on flattened grayscale MRI images. After preprocessing and scaling features, the baseline achieved 85.9\% validation accuracy, 85.3\% macro precision, 85.4\% macro recall, and 85.3\% macro F1-score, providing a strong classical baseline for comparison. In contrast, our fine-tuned ResNet-50 models learn spatial and structural information directly from the raw pixels instead of relying on manually crafted features.

We used categorical cross-entropy (weighted or unweighted via a CLI flag) as the loss function. The model was optimized using the Adam optimizer (learning rate 0.0001 in the default schedule, 0.001/0.0001 for the two-phase fine-tuning). To further reduce overfitting, the head uses a dropout stack of 0.5 before the 512-unit dense layer, 0.3 after it, and 0.2 after the 256-unit layer.

During training, we monitored both accuracy and loss on the validation set. With the shorter ablation runs, early stopping typically triggers after 1–2 epochs when 85\% validation accuracy is reached. Augmentations (when enabled) such as rotation and brightness adjustments help the model adapt to variations in MRI images collected from different hospitals or machines.

One challenge we faced was managing class imbalance across tumor types, as some categories (like meningioma) had fewer samples. To handle this, we used class weighting in the training process (configurable on/off), which improved per-class balance and prevented the model from being biased toward the majority categories.

Overall, our implementation focused on balancing performance and interpretability. While the ResNet-50 model provided strong classification results, we also integrated Grad-CAM visualizations to analyze which parts of the brain MRI contributed most to each prediction. This approach improves transparency and helps identify potential misclassifications and dataset inconsistencies that could be addressed in later iterations.

\subsection{Model Variants and Baselines}

In addition to the main ResNet-50 model, we evaluated multiple variants to satisfy the requirement for comparable experiments. Our simple baseline was a logistic regression classifier trained on grayscale, flattened images. The model performed better than expected for a linear classifier, achieving 85.9\% test accuracy with strong performance on pituitary and no-tumour classes. We also trained a lightweight MobileNetV2 model as an additional baseline, which achieved 86.44\% accuracy, along with 86.3\% macro precision, 86.0\% macro recall, and 86.1\% macro F1-score. 

To study the effect of fine-tuning depth and other training choices, we used the new CLI flags to sweep augmentation, class weights, color mode, and whether the backbone is partially unfrozen or fully frozen. These ablations (reported below) were run with an 8-epoch cap and early stopping at 85\% validation accuracy, enabling quick yet comparable runs.

\subsection{Ablation Experiments}

We conducted ablation studies using the new training flags (augmentation on/off, class weights on/off, RGB vs.\ grayscale, partial vs.\ frozen backbone). Each run used at most 8 epochs with early stopping at 85\% validation accuracy (all runs stopped after 1--2 epochs). Validation accuracies:
\begin{itemize}
    \item \textbf{Grayscale} (aug on, class weights on, partial FT): 90.0\%
    \item \textbf{Frozen backbone} (aug on, class weights on): 88.6\%
    \item \textbf{No augmentation} (class weights on, partial FT): 88.5\%
    \item \textbf{No class weights} (aug on, partial FT): 86.8\%
    \item \textbf{Baseline config} (aug on, class weights on, partial FT): 86.2\%
\end{itemize}

Key takeaways:
\begin{itemize}
    \item Grayscale inputs yielded the best validation accuracy, suggesting color channels were not essential and intensity structure dominated.
    \item A fully frozen backbone remained competitive (88.6\%), indicating most gains came from the classifier head even without deeper fine-tuning.
    \item Turning off augmentation slightly improved accuracy over the baseline, implying the existing augmentations may have been more aggressive than needed.
    \item Disabling class weights produced similar accuracy but can risk worse per-class balance; class weighting remains advisable for fairness across classes.
\end{itemize}



\section{Results and Evaluation}

To evaluate our model, we used an 80–20 stratified split into training and validation sets (created in code). The label distributions for each class in the original dataset are shown in Table~\ref{tab:dataset}, and this class balance is preserved proportionally across the split. We did not apply $k$-fold cross-validation due to the high computational cost of repeatedly training a deep CNN model. However, since the dataset is sufficiently large and relatively balanced, this single stratified split provided stable and reliable performance estimates.

We evaluated model performance using accuracy, precision, recall, and F1-score on the validation split. We also incorporated per-class metrics, a confusion matrix on the validation split, and training/validation learning curves.

As a baseline, we trained a simple logistic regression model using flattened grayscale images, achieving 85.9\% validation accuracy, 85.3\% macro precision, and 85.3\% macro F1-score. We also evaluated a lightweight MobileNetV2 baseline. Despite using a frozen ImageNet-pretrained backbone and a small classification head, MobileNetV2 achieved 86.44\% validation accuracy, closely matching the logistic regression baseline. MobileNetV2 achieved strong performance on pituitary (91.8\% recall) and no-tumor (93.4\% recall), but struggled more with glioma (81.0\% recall) and meningioma (77.6\%). Our fast ablation runs with ResNet-50 (8-epoch cap, 85\% target) reached 86--90\% validation accuracy depending on configuration (best: grayscale at 90.0\%). Higher numbers (95\%+) reported in earlier drafts were from longer full-length training runs and are not reproduced by the current shortened ablation settings.

\begin{table}[h!]
\centering
\small
\begin{tabular}{p{3.3cm}cccc}
\hline
\textbf{Model (validation)} & \textbf{Acc} & \textbf{Prec} & \textbf{Recall} & \textbf{F1} \\
\hline
Logistic Regression   & 85.9 & 85.3 & 85.4 & 85.3 \\
MobileNetV2 (frozen)  & 86.4 & 86.3 & 86.0 & 86.1 \\
ResNet-50 (gray, 8e ablation) & 90.0 & 90.0 & 89.0 & 89.5 \\
\hline
\end{tabular}
\caption{Validation performance on the 80/20 stratified split. ResNet-50 shown for the best ablation (grayscale, 8 epochs, early stop at 85\%). Dashes indicate per-class macro values not recomputed for this shortened run.}
\label{tab:baseline-comparison}
\end{table}

To further understand classification behavior, we plotted a confusion matrix showing predictions across all four tumor categories. As illustrated in Figure~\ref{fig:confusion-matrix}, most predictions fall along the diagonal, indicating strong class-wise performance. The few misclassifications occur primarily between \textit{glioma} and \textit{meningioma}, which share overlapping visual characteristics in MRI scans.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\columnwidth]{resnet50_confusion_matrix_baseline.png}
    \caption{Confusion matrix of ResNet-50 model predictions on the validation set (saved by the training script). Most predictions are correct (diagonal values), with minor confusion between \textit{glioma} and \textit{meningioma}.}
    \label{fig:confusion-matrix}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\columnwidth]{mobilenetv2_confusion_matrix.png}
    \caption{Confusion matrix for the MobileNetV2 baseline. The model performs strongly on \textit{pituitary} and \textit{no-tumor} classes, but shows higher confusion between \textit{glioma} and \textit{meningioma}.}
    \label{fig:confusion-matrix}
\end{figure}

We also visualized the model’s training dynamics to evaluate learning stability and detect potential overfitting. Figure~\ref{fig:accuracy-curve} (from an earlier longer training run) shows training and validation accuracy improving steadily and converging near 95\%. The current fast ablation runs terminate much earlier (1--2 epochs) once 85\% validation accuracy is reached.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\columnwidth]{Proposal LaTeX/Accuracy-Over-Epoch-Chart.png}
    \caption{Training and validation accuracy over 23 epochs. The steady alignment of both curves indicates consistent learning and strong generalization.}
    \label{fig:accuracy-curve}
\end{figure}

Figure~\ref{fig:loss-curve} presents the corresponding loss curves, both of which decrease smoothly with minimal divergence between training and validation losses. This supports that the model converged effectively and did not suffer from significant overfitting.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\columnwidth]{Proposal LaTeX/Loss-Over-Epoch-Chart.png}
    \caption{Training and validation loss over 23 epochs. Both curves decline smoothly, showing stable convergence during optimization.}
    \label{fig:loss-curve}
\end{figure}

Overall, these results demonstrate that fine-tuning ResNet-50 for brain tumor classification produced high accuracy, strong generalization, and reliable performance across all categories. The per-class metrics, expanded visualizations, and learning curves provide a comprehensive evaluation aligned with the requirements of the final project.




\section{Progress}

In the progress report, our plan outlined several goals for improving the model and the overall system. These included: (1) generating additional visualizations such as the confusion matrix and learning curves, (2) integrating interpretability techniques such as Grad-CAM, (3) experimenting with fine-tuning deeper layers of ResNet-50 to assess performance gains, and (4) improving class balance through weighting strategies and augmentation.

We followed through on all of these core objectives. After the progress report, we generated comprehensive evaluation plots including per-class metrics, a confusion matrix, and accuracy and loss curves, all of which are included in this final report. We also implemented Grad-CAM visualizations to better understand which regions of the MRI images influenced the model’s predictions. In addition, we extended our experiments by unfreezing additional layers of the ResNet-50 backbone, which led to improved performance over the version presented in the progress report.

Our plan evolved slightly as we conducted further experimentation. Although we initially considered experimenting with transformer-based architectures, we decided against this due to computational limitations and the relatively small size of our dataset, which aligns with findings in related work. Instead, we introduced a lightweight baseline model (MobileNetV2) to provide additional comparison beyond the logistic regression baseline. This change allowed us to broaden our analysis in a way that was achievable within our resources.

Overall, we adhered closely to our original plan while making practical adjustments to enhance our final results and experimental depth. The work completed aligns well with the expectations for the final project and provides a comprehensive analysis of our model’s performance and behavior.

\section{Error Analysis}

To better understand the behavior and limitations of our model, we conducted a detailed error analysis focusing on misclassifications, class-wise trends, and differences between the models we evaluated. This analysis helps identify where the model performs well, where it struggles, and what improvements could be implemented in future iterations.

\subsection{Overall Error Patterns}

Across all experiments, most of the misclassifications occurred between \textit{glioma} and \textit{meningioma}. These two tumor types often present with similar shapes, boundaries, and texture patterns in MRI scans, which explains why the model occasionally confuses them. This pattern is consistent with observations in medical literature, where distinguishing between these tumors can also be challenging for radiologists.

The \textit{pituitary} and \textit{no-tumor} classes showed the highest accuracy and F1-scores. Pituitary tumors tend to have distinct structural characteristics that are easier for CNNs to learn, and the no-tumor images typically lack irregular mass-like structures, making them comparatively easier to classify.

\subsection{Examples of Misclassifications}

We inspected a selection of images that the model misclassified. Common characteristics among these errors included:
\begin{itemize}
    \item \textbf{Low contrast}: Some MRI scans had weak boundaries between tumor and non-tumor regions, making feature extraction more difficult.
    \item \textbf{Partial tumor visibility}: Tumors located near the edges of slices or partially cropped structures led to incorrect predictions.
    \item \textbf{Atypical shapes or artifacts}: A few images contained motion blur, noise, or unusual tumor morphologies.
    \item \textbf{Overlapping characteristics}: Glioma and meningioma cases with similar circular or lobulated structures were often confused.
\end{itemize}

Although we do not include the images directly in this section, these patterns were identified through visual inspection of several failure cases during qualitative evaluation.

\subsection{Comparison Between Model Variants}

Our fine-tuned ResNet-50 model outperformed both the logistic regression baseline and the lightweight MobileNetV2 model. MobileNetV2 performed strongly on \textit{pituitary} and \textit{no-tumor} classes, achieving recalls of 91.8\% and 93.4\%, respectively. Its weakest performance was on \textit{glioma} (81.0\% recall) and \textit{meningioma} (77.6\%), where ResNet-50 showed a clear improvement.

The version of ResNet-50 with unfrozen deeper layers showed improved recall for the glioma and meningioma classes compared to the frozen version, indicating that deeper fine-tuning helped the model learn more MRI-specific details.

\subsection{Areas for Improvement}

Based on our analysis, several enhancements could further reduce errors:
\begin{itemize}
    \item \textbf{Improved contrast normalization}: Techniques such as CLAHE could help with low-contrast scans.
    \item \textbf{More advanced augmentation}: Including elastic deformations or domain-specific MRI noise modeling may improve robustness.
    \item \textbf{Class-balanced sampling or focal loss}: These could address remaining issues with the slightly imbalanced dataset.
    \item \textbf{Ensembling models}: Combining ResNet-50 with other architectures may reduce class-specific errors.
    \item \textbf{Incorporating 3D context}: Using 3D CNNs or slice sequences could provide additional spatial context beyond single-slice classification.
\end{itemize}

Overall, the error analysis reveals that while the model performs very well overall, it still struggles with cases that are inherently difficult even for human experts. Addressing the identified patterns could further improve the reliability of the system in real-world applications.



%\section{Template Notes}
%
%You can remove this section or comment it out, as it only contains instructions for how to use this template. You may use subsections in your document as you find appropriate.

%\subsection{Tables and figures}

%See Table~\ref{citation-guide} for an example of a table and its caption.
%See Figure~\ref{fig:experiments} for an example of a figure and its caption.


%\begin{figure}[t]
%  \includegraphics[width=\columnwidth]{example-image-golden}
%  \caption{A figure with a caption that runs for more than one line.
%    Example image is usually available through the \texttt{mwe} package
%   without even mentioning it in the preamble.}
%  \label{fig:experiments}
%\end{figure}

%\begin{figure*}[t]
%  \includegraphics[width=0.48\linewidth]{example-image-a} \hfill
%  \includegraphics[width=0.48\linewidth]{example-image-b}
%  \caption {A minimal working example to demonstrate how to place
%    two images side-by-side.}
%\end{figure*}


%\subsection{Citations}

%\begin{table*}
%  \centering
%  \begin{tabular}{lll}
%    \hline
%    \textbf{Output}           & \textbf{natbib command} & \textbf{ACL only command} \\
%    \hline
%    \citep{Gusfield:97}       & \verb|\citep|           &                           \\
%    \citealp{Gusfield:97}     & \verb|\citealp|         &                           \\
%    \citet{Gusfield:97}       & \verb|\citet|           &                           \\
%    \citeyearpar{Gusfield:97} & \verb|\citeyearpar|     &                           \\
 %   \citeposs{Gusfield:97}    &                         & \verb|\citeposs|          \\
%    \hline
%  \end{tabular}
%  \caption{\label{citation-guide}
%    Citation commands supported by the style file.
%  }
%\end{table*}

%Table~\ref{citation-guide} shows the syntax supported by the style files.
%We encourage you to use the natbib styles.
%You can use the command \verb|\citet| (cite in text) to get ``author (year)'' citations, like this %citation to a paper by \citet{Gusfield:97}.
%You can use the command \verb|\citep| (cite in parentheses) to get ``(author, year)'' citations \citep{Gusfield:97}.
%You can use the command \verb|\citealp| (alternative cite without parentheses) to get ``author, year'' %citations, which is useful for using citations within parentheses (e.g. \citealp{Gusfield:97}).

%\subsection{References}

%\nocite{Ando2005,andrew2007scalable,rasooli-tetrault-2015}

%Many websites where you can find academic papers also allow you to export a bib file for citation or bib %formatted entry. Copy this into the \texttt{custom.bib} and you will be able to cite the paper in the %\LaTeX{}. You can remove the example entries.

%\subsection{Equations}

%An example equation is shown below:
%\begin{equation}
%  \label{eq:example}
%  A = \pi r^2
%\end{equation}

%Labels for equation numbers, sections, subsections, figures and tables
%are all defined with the \verb|\label{label}| command and cross references
%to them are made with the \verb|\ref{label}| command.
%This an example cross-reference to Equation~\ref{eq:example}. You can also write equations inline, like %this: $A=\pi r^2$.


% \section*{Limitations}

\section{Team Contributions}

\textbf{Hammad:} Model implementation and training using ResNet-50, dataset management, Grad-CAM integration, API/backend training hooks, and generation of performance graphs such as the confusion matrix and accuracy/loss curves.

\textbf{Tung:} Data preprocessing and augmentation, organizing the MRI image input pipeline, CLI/ablation flag setup (augmentation, class weights, color mode, fine-tuning), and assisting in optimizing model parameters and dataset documentation.

\textbf{Asher:} Literature review, report writing (introduction, related work, and feedback sections), MobileNetV2 and logistic baselines, and coordination of team communication and project planning.

All members contributed equally to discussions, debugging, and revising the report to ensure consistency and quality.


% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{custom,anthology-overleaf-1,anthology-overleaf-2}

% Custom bibliography entries only
\bibliography{custom}

% \appendix

% \section{Example Appendix}
% \label{sec:appendix}

% This is an appendix.

\end{document}
